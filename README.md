# Axiom - High-Performance Tensor Library for Apple Silicon
A state-of-the-art C++ tensor library optimized for Apple Silicon, providing full NumPy syntax compatibility and einops-style tensor rearrangement for seamless on-device AI inference.

## Features
- ğŸš€ Apple Silicon Optimized - Leverages Metal Performance Shaders and ARM NEON vectorization
- ğŸ NumPy Compatible - Full syntax compatibility for easy Python-to-C++ translation
- ğŸ”„ Einops Support - Intuitive tensor rearrangement with einops-style operations
- ğŸ“Š ONNX Ready - Seamless integration with ONNX model inference pipelines
- âš¡ Zero-Copy Operations - Memory-efficient tensor operations where possible
- ğŸ¯ Type Safety - Modern C++20 with compile-time shape checking

## Quick Start
### Prerequisites
- macOS 11.0+ (Big Sur) with Apple Silicon
- Xcode 13+ or Clang 13+
- CMake 3.20+
- Metal Performance Shaders Framework

## Build Instructions
```bash
# Clone the repository
git clone https://github.com/your-username/axiom.git
cd axiom

# Create build directory
mkdir build && cd build

# Configure with CMake
cmake .. -DCMAKE_BUILD_TYPE=Release -DAXIOM_BUILD_TESTS=ON

# Build
make -j$(sysctl -n hw.ncpu)

# Run tests
make test
```

## Installation
```bash
# Install system-wide
sudo make install

# Or use in your CMake project
find_package(Axiom REQUIRED)
target_link_libraries(your_target Axiom::axiom)
```

## Usage Example
```cpp
#include <axiom/tensor.hpp>
#include <axiom/ops.hpp>

using namespace axiom;

int main() {
    // Create tensors with NumPy-like syntax
    auto x = tensor::randn({64, 128, 256});
    auto y = tensor::ones({256, 512});
    
    // Matrix operations
    auto result = x.matmul(y);
    
    // Einops-style rearrangement
    auto reshaped = x.rearrange("b h w -> b (h w)");
    
    // Broadcasting and element-wise ops
    auto scaled = (x * 2.0f + 1.0f).relu();
    
    return 0;
}
```

## Project Structure
```
axiom/
â”œâ”€â”€ CMakeLists.txt              # Main CMake configuration
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ LICENSE                     # MIT License
â”œâ”€â”€ include/                    # Public headers
â”‚   â””â”€â”€ axiom/
â”‚       â”œâ”€â”€ tensor.hpp          # Core tensor class
â”‚       â”œâ”€â”€ ops.hpp            # Tensor operations
â”‚       â”œâ”€â”€ einops.hpp         # Einops-style rearrangements
â”‚       â”œâ”€â”€ metal_backend.hpp  # Metal backend interface
â”‚       â””â”€â”€ numpy_compat.hpp   # NumPy compatibility layer
â”œâ”€â”€ src/                       # Implementation files
â”‚   â”œâ”€â”€ tensor/
â”‚   â”‚   â”œâ”€â”€ tensor.cpp         # Core tensor implementation
â”‚   â”‚   â”œâ”€â”€ shape.cpp          # Shape and stride handling
â”‚   â”‚   â””â”€â”€ storage.cpp        # Memory management
â”‚   â”œâ”€â”€ ops/
â”‚   â”‚   â”œâ”€â”€ arithmetic.cpp     # Basic arithmetic operations
â”‚   â”‚   â”œâ”€â”€ linalg.cpp         # Linear algebra operations
â”‚   â”‚   â”œâ”€â”€ reduction.cpp      # Reduction operations
â”‚   â”‚   â””â”€â”€ indexing.cpp       # Indexing and slicing
â”‚   â”œâ”€â”€ einops/
â”‚   â”‚   â”œâ”€â”€ parser.cpp         # Einstein notation parser
â”‚   â”‚   â””â”€â”€ rearrange.cpp      # Rearrangement implementation
â”‚   â””â”€â”€ backends/
â”‚       â”œâ”€â”€ metal/
â”‚       â”‚   â”œâ”€â”€ metal_backend.cpp
â”‚       â”‚   â”œâ”€â”€ kernels.metal   # Metal compute shaders
â”‚       â”‚   â””â”€â”€ metal_utils.cpp
â”‚       â””â”€â”€ cpu/
â”‚           â”œâ”€â”€ cpu_backend.cpp
â”‚           â””â”€â”€ vectorized_ops.cpp
â”œâ”€â”€ tests/                     # Unit tests
â”‚   â”œâ”€â”€ test_tensor.cpp
â”‚   â”œâ”€â”€ test_ops.cpp
â”‚   â”œâ”€â”€ test_einops.cpp
â”‚   â””â”€â”€ test_numpy_compat.cpp
â”œâ”€â”€ examples/                  # Example usage
â”‚   â”œâ”€â”€ basic_usage.cpp
â”‚   â”œâ”€â”€ onnx_inference.cpp
â”‚   â””â”€â”€ performance_benchmark.cpp
â”œâ”€â”€ benchmarks/                # Performance benchmarks
â”‚   â”œâ”€â”€ benchmark_ops.cpp
â”‚   â””â”€â”€ compare_numpy.py
â””â”€â”€ cmake/                     # CMake modules
    â”œâ”€â”€ FindMetal.cmake
    â””â”€â”€ AxiomConfig.cmake.in
```
## Development Roadmap
### Phase 1: Core Foundation

[ ] Basic tensor class with shape/stride handling
[ ] Memory management and storage abstraction
[ ] CMake build system setup
[ ] Basic arithmetic operations (+, -, *, /)

### Phase 2: Operations & Backends

[ ] Metal backend implementation
[ ] Linear algebra operations (matmul, dot, etc.)
[ ] Reduction operations (sum, mean, max, etc.)
[ ] Broadcasting semantics

### Phase 3: Advanced Features

[ ] Einops-style rearrangement parser
[ ] NumPy compatibility layer
[ ] Automatic differentiation (optional)
[ ] ONNX integration utilities

### Phase 4: Optimization

[ ] Memory pool allocation
[ ] Kernel fusion optimizations
[ ] Multi-threading for CPU operations
[ ] Benchmarking and profiling tools

## Performance Goals

- Memory Bandwidth: >90% of theoretical peak on Apple Silicon
- Compute Utilization: >85% GPU utilization for large tensor operations
- Latency: <1ms overhead for tensor creation and basic operations
- Compatibility: 100% pass rate on NumPy compatibility test suite

## License
MIT License - see [LICENSE](LICENSE) file for details.

## Citation
```bibtex
@misc{axiom2025,
  title={Axiom: High-Performance Tensor Library for Apple Silicon},
  author={Noah Kay},
  year={2025},
  url={https://github.com/frikallo/axiom}
}
```